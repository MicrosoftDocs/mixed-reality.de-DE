---
title: Eyetracking – Blickverfolgung
description: Eyetracking – Blickverfolgung
author: sostel
ms.author: sostel
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: Eyetracking, Blickverfolgung, Mixed Reality, Eingabe, Anvisieren mit den Augen
ms.openlocfilehash: 7298a34a946f86aaf789cfe44ad971169fc8ece3
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 06/05/2019
ms.locfileid: "66453699"
---
# <a name="eye-tracking-on-hololens-2"></a><span data-ttu-id="2568f-104">Blickverfolgung auf HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="2568f-104">Eye tracking on HoloLens 2</span></span>
<span data-ttu-id="2568f-105">Mit HoloLens 2 erschließt sich in Bezug auf Kontext und menschliches Verständnis eine ganz neue Ebene der holografischen Erfahrung. Das Gerät bietet Entwicklern nämlich die unglaubliche Möglichkeit, Informationen zur Zielanvisierung mit den Augen und zur Blickbewegung des Benutzers zu verwenden.</span><span class="sxs-lookup"><span data-stu-id="2568f-105">HoloLens 2 allows for a whole new level of context and human understanding within the holographic experience by providing developers with the incredible ability of using information about what users are looking at.</span></span> <span data-ttu-id="2568f-106">Diese Seite gibt einen Überblick darüber, wie Entwickler bei verschiedenen Anwendungsfällen von der Blickverfolgung profitieren können und was sie beim Entwerfen von Benutzeroberflächen beachten müssen, bei denen das Anvisieren mit den Augen die Grundlage bildet.</span><span class="sxs-lookup"><span data-stu-id="2568f-106">This page gives an overview of how developers can benefit from eye tracking for various use cases and what to look out for when designing eye-gaze-based user interfaces.</span></span> 

## <a name="use-cases"></a><span data-ttu-id="2568f-107">Anwendungsfälle</span><span class="sxs-lookup"><span data-stu-id="2568f-107">Use cases</span></span>
<span data-ttu-id="2568f-108">Mit der Blickverfolgung können Anwendungen in Echtzeit verfolgen, wohin der Benutzer schaut.</span><span class="sxs-lookup"><span data-stu-id="2568f-108">Eye tracking enables applications to track where the user is looking in real time.</span></span> <span data-ttu-id="2568f-109">In diesem Abschnitt werden einige mögliche Anwendungsfälle und neuartige Interaktionen beschrieben, die in der Mixed Reality-Umgebung mit der Blickverfolgung möglich sind.</span><span class="sxs-lookup"><span data-stu-id="2568f-109">This section describes some of the potential use cases and novel interactions that become possible with eye tracking in mixed reality.</span></span>
<span data-ttu-id="2568f-110">Bevor Sie beginnen: Im Folgenden wird mehrmals der [Mixed Reality-Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) erwähnt, da mit ihm mehrere interessante und leistungsfähige Beispiele für die Blickverfolgung bereitgestellt werden, wie beispielsweise die schnelle und mühelose blickgestützte Zielauswahl und das automatische Scrollen durch Text anhand des Blickverlaufs des Benutzers.</span><span class="sxs-lookup"><span data-stu-id="2568f-110">Before getting started, in the following we will mention the [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) several times as it provides several interesting and powerful examples for using eye tracking such as quick and effortless eye-supported target selections and automatically scrolling through text based on where the user looks at.</span></span> 

### <a name="user-intent"></a><span data-ttu-id="2568f-111">Benutzerabsicht</span><span class="sxs-lookup"><span data-stu-id="2568f-111">User intent</span></span>    
<span data-ttu-id="2568f-112">Die Informationen zu den Punkten, auf die der Benutzer schaut, bilden einen leistungsfähigen **Kontext für andere Eingaben** (z.B. Sprach- und Handeingaben und Eingaben über den Controller).</span><span class="sxs-lookup"><span data-stu-id="2568f-112">Information about where a user looks at provides a powerful **context for other inputs**, such as voice, hands and controllers.</span></span>
<span data-ttu-id="2568f-113">Dies kann für verschiedene Aufgaben verwendet werden.</span><span class="sxs-lookup"><span data-stu-id="2568f-113">This can be used for various tasks.</span></span>
<span data-ttu-id="2568f-114">Das beginnt beispielsweise bei der schnellen und mühelosen **Zieladressierung** in der Szene, indem der Benutzer einfach ein Hologramm anvisiert und „Auswählen“ sagt (siehe auch [Anvisieren mit dem Kopf und Ausführen](gaze-and-commit.md)). Oder der Benutzer sagt „Auswählen, Verschieben“, visiert dann den Punkt an, an dem das Hologramm platziert werden soll, und sagt „...hierhin“.</span><span class="sxs-lookup"><span data-stu-id="2568f-114">For example, this may range from quickly and effortlessly **targeting** across the scene by simply looking at a hologram and saying "select" (also see [Head-gaze and commit](gaze-and-commit.md)) or by saying "put this...", then looking over to where you want to place the hologram and say "...there".</span></span> <span data-ttu-id="2568f-115">Beispiele hierfür finden Sie in den Artikeln [Mixed Reality Toolkit – Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) (Mixed Reality-Toolkit – Blickgestützte Zielauswahl) und [Mixed Reality Toolkit – Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html) (Mixed Reality-Toolkit – Blickgestützte Zielpositionierung).</span><span class="sxs-lookup"><span data-stu-id="2568f-115">Examples for this can be found in [Mixed Reality Toolkit - Eye-supported Target Selection](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_TargetSelection.html) and [Mixed Reality Toolkit - Eye-supported Target Positioning](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Positioning.html).</span></span>

<span data-ttu-id="2568f-116">Ein weiteres Beispiel für die Benutzerabsicht ist beispielsweise die Nutzung der Informationen zu den Punkten, auf die ein Benutzer schaut, um den Einsatz der enthaltenen virtuellen Agents und interaktiven Hologramme zu verbessern.</span><span class="sxs-lookup"><span data-stu-id="2568f-116">An additional example for user intent may include using information about what users look at to enhance the engagement with embodied virtual agents and interactive holograms.</span></span> <span data-ttu-id="2568f-117">Virtuelle Agents können beispielsweise die verfügbaren Optionen und deren Verhalten anhand des aktuell anvisierten Inhalts anpassen.</span><span class="sxs-lookup"><span data-stu-id="2568f-117">For example, virtual agents may adapt available options and their behavior based on currently viewed content.</span></span> 

### <a name="implicit-actions"></a><span data-ttu-id="2568f-118">Implizite Aktionen</span><span class="sxs-lookup"><span data-stu-id="2568f-118">Implicit actions</span></span>
<span data-ttu-id="2568f-119">Die Kategorie der impliziten Aktionen steht in enger Beziehung zur Benutzerabsicht.</span><span class="sxs-lookup"><span data-stu-id="2568f-119">The category of implicit actions closely relates to user intent.</span></span>
<span data-ttu-id="2568f-120">Die Idee ist, dass Hologramme oder Elemente der Benutzeroberfläche auf eine mehr instinktive Art und Weise reagieren, wodurch vielleicht nicht einmal der Eindruck entsteht, dass der Benutzer mit dem System interagiert, sondern dass System und Benutzer synchron arbeiten. Ein besonders erfolgreiches Beispiel hierfür ist das **automatische Scrollen durch Anvisieren mit den Augen**.</span><span class="sxs-lookup"><span data-stu-id="2568f-120">The idea is that holograms or user interface elements react in a somewhat instinctual way that may not even feel like you are interacting with the system at all, but rather that the system and the user are in sync. For example, one immensely successful example is **eye-gaze-based auto scroll**.</span></span> <span data-ttu-id="2568f-121">Die Idee ist einfach: Der Benutzer liest einen Test und kann einfach weiter lesen.</span><span class="sxs-lookup"><span data-stu-id="2568f-121">The idea is as simple: The user reads a text and can just keep on reading.</span></span> <span data-ttu-id="2568f-122">Der Text verschiebt sich allmählich nach oben, um den Lesefluss des Benutzers zu unterstützen.</span><span class="sxs-lookup"><span data-stu-id="2568f-122">The text gradually moves up to keep users in their reading flow.</span></span> <span data-ttu-id="2568f-123">Ein wichtiger Aspekt ist, dass sich die Scrollgeschwindigkeit an die Lesegeschwindigkeit des Benutzers anpasst.</span><span class="sxs-lookup"><span data-stu-id="2568f-123">A key aspect is that the scrolling speed adapts to the reading speed of the user.</span></span>
<span data-ttu-id="2568f-124">Ein weiteres Beispiel ist das **blickgestützte Zoomen und Schwenken**, bei dem der Benutzer das Gefühl hat, genau in das einzutauchen, worauf er sich konzentriert.</span><span class="sxs-lookup"><span data-stu-id="2568f-124">Another example is **eye-supported zoom and pan** for which the user can feel like diving exactly toward what he or she is focusing at.</span></span> <span data-ttu-id="2568f-125">Das Auslösen des Zoomvorgangs und das Steuern der Zoomgeschwindigkeit kann über Sprach- oder Handeingaben erfolgen, was wichtig ist, um dem Benutzer ein Gefühl der Kontrolle zu vermitteln und eine Überforderung zu vermeiden (diese Entwurfsrichtlinien werden im Detail weiter unten behandelt).</span><span class="sxs-lookup"><span data-stu-id="2568f-125">Triggering the zoom and controlling the zoom speed can be controlled via voice or hand input which is important about providing the feeling of control and avoid overwhelming the user (we will talk about these design guidelines in more detail below).</span></span> <span data-ttu-id="2568f-126">Nach dem Vergrößern kann der Benutzer dann nahtlos durch Anvisieren mit den Augen zum Beispiel einem Straßenverlauf folgen und seine Umgebung erkunden.</span><span class="sxs-lookup"><span data-stu-id="2568f-126">Once zoomed in, the user can then smoothly follow, for example, the course of a street to explore his or her neighborhood just simply by using their eye gaze.</span></span>
<span data-ttu-id="2568f-127">Demobeispiele für diese Arten von Interaktion finden Sie im Beispiel [Mixed Reality Toolkit – Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) (Mixed Reality-Toolkit – Blickgestützte Navigation).</span><span class="sxs-lookup"><span data-stu-id="2568f-127">Demo examples for these types of interactions can be found in the [Mixed Reality Toolkit - Eye-supported Navigation](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Navigation.html) sample.</span></span>

<span data-ttu-id="2568f-128">Weitere Anwendungsfälle für _implizite Aktionen_:</span><span class="sxs-lookup"><span data-stu-id="2568f-128">Additional use cases for _implicit actions_ may include:</span></span>
- <span data-ttu-id="2568f-129">**Intelligente Benachrichtigungen**: Ärgern Sie sich auch jedes Mal, wenn Benachrichtigungen dort angezeigt werden, wohin Sie gerade schauen?</span><span class="sxs-lookup"><span data-stu-id="2568f-129">**Smart notifications:** Ever get annoyed by notifications popping up right where you were focusing?</span></span> <span data-ttu-id="2568f-130">Wenn Sie berücksichtigen, worauf die Aufmerksamkeit eines Benutzers gerichtet ist, können Sie es besser machen!</span><span class="sxs-lookup"><span data-stu-id="2568f-130">Taking into account where a user is currently paying attention to, you can make it better!</span></span> <span data-ttu-id="2568f-131">Zeigen Sie Benachrichtigungen mit einem Offset von der Stelle an, auf die der Benutzer aktuell schaut, um Ablenkungen auf ein Minimum zu beschränken. Schließen Sie die Benachrichtigungen automatisch, nachdem der Benutzer sie zu Ende gelesen hat.</span><span class="sxs-lookup"><span data-stu-id="2568f-131">Show notifications offset from where the user is currently looking to limit distractions and automatically dismiss them once finished reading.</span></span> 
- <span data-ttu-id="2568f-132">**„Aufmerksame“ Hologramme:** Hologramme können fast unmerklich reagieren, wenn sie angeschaut werden.</span><span class="sxs-lookup"><span data-stu-id="2568f-132">**Attentive holograms:** Holograms that subtly react when being looked at.</span></span> <span data-ttu-id="2568f-133">Dies können leicht erglühende Oberflächenelemente, eine langsam erblühende Blume oder ein virtuelles Haustier sein, das beginnt, Ihren Blick zu erwidern, oder das nach einer Weile des Anvisierens den Blick abwendet.</span><span class="sxs-lookup"><span data-stu-id="2568f-133">This may range from slightly glowing UI elements, a slowly blooming flower to a virtual pet starting to look back at you or trying to avoid your eye gaze after a prolonged stare.</span></span> <span data-ttu-id="2568f-134">Dies kann in Ihrer App zu einem interessanten Gefühl von Verbundenheit und Zufriedenheit führen.</span><span class="sxs-lookup"><span data-stu-id="2568f-134">This may provide an interesting sense of connectivity and satisfaction in your app.</span></span>

### <a name="attention-tracking"></a><span data-ttu-id="2568f-135">Aufmerksamkeitsverfolgung</span><span class="sxs-lookup"><span data-stu-id="2568f-135">Attention tracking</span></span>   
<span data-ttu-id="2568f-136">Informationen zu den Punkten, auf die der Benutzer schaut, stellen ein unglaublich leistungsfähiges Tool zur Bewertung der Benutzerfreundlichkeit von Designs und zum Identifizieren von Problemen in effizienten Arbeitsabläufen dar.</span><span class="sxs-lookup"><span data-stu-id="2568f-136">Information about where users look at is an immensely powerful tool to assess usability of designs and to identify problems in efficient work streams.</span></span> <span data-ttu-id="2568f-137">Mittlerweile sind Visualisierung und Analyse mittels Blickverfolgung in verschiedenen Anwendungsbereichen bereits gängige Praxis.</span><span class="sxs-lookup"><span data-stu-id="2568f-137">By now,  eye tracking visualization and analytics are already a common practice in various application areas.</span></span> <span data-ttu-id="2568f-138">Mit HoloLens 2 stellen wir eine neue Dimension für diese Grundlagen bereit, da 3D-Hologramme in reale Kontexte platziert und parallel mit diesen bewertet werden können.</span><span class="sxs-lookup"><span data-stu-id="2568f-138">With HoloLens 2, we provide a new dimension to this understanding as 3D holograms can be placed in real-world contexts and assessed alongside.</span></span> <span data-ttu-id="2568f-139">Der [Mixed Reality-Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) enthält grundlegende Beispiele für das Protokollieren und Laden von Blickverfolgungsdaten sowie für das Visualisieren dieser Daten.</span><span class="sxs-lookup"><span data-stu-id="2568f-139">The [Mixed Reality Toolkit](https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html) provides basic examples for logging and loading eye tracking data and for how to visualize them.</span></span>

<span data-ttu-id="2568f-140">Zu diesem Bereich zählen möglicherweise auch die folgenden Anwendungen:</span><span class="sxs-lookup"><span data-stu-id="2568f-140">Other applications in this area may include:</span></span> 
-   <span data-ttu-id="2568f-141">**Visualisieren des entfernten Anvisierens mit den Augen:** Visualisieren Sie, worauf entfernte Projektmitarbeiter schauen, um z.B. sicherzustellen, dass Anweisungen richtig verstanden und befolgt werden.</span><span class="sxs-lookup"><span data-stu-id="2568f-141">**Remote eye gaze visualization:** Visualize what remote collaborators are looking at to, for example, ensure whether instructions are correctly understood and followed.</span></span>
-   <span data-ttu-id="2568f-142">**Forschungsstudien zum Benutzerverhalten:** Anhand der Aufmerksamkeitsverfolgung kann untersucht werden, wie unerfahrene Benutzer im Vergleich zu erfahrenen Benutzern Inhalte visuell analysieren oder bei komplexen Aufgaben (z.B. Analyse medizinischer Daten oder beim Betrieb von Maschinen) die Hand-/Augeninteraktion koordinieren.</span><span class="sxs-lookup"><span data-stu-id="2568f-142">**User research studies:** Attention tracking can be used to explore the way novice vs. experts users visually analyze content or their hand-eye-coordination for complex tasks (e.g., for analysis of medical data or while operating machinery).</span></span>
-   <span data-ttu-id="2568f-143">**Schulungssimulationen und Leistungsüberwachung:** Üben Sie die Ausführung von Aufgaben, und optimieren Sie diese, indem Sie Engpässe im Ausführungsablauf effektiver identifizieren.</span><span class="sxs-lookup"><span data-stu-id="2568f-143">**Training simulations and Performance monitoring:** Practice and optimize the execution of tasks by identifying bottlenecks more effectively in the execution flow.</span></span>
-   <span data-ttu-id="2568f-144">**Entwurfsbewertungen, Werbung und Marktforschung:** Die Blickverfolgung ist ein gängiges Tool für die Marktforschung, um Website -und Produktentwürfe zu bewerten.</span><span class="sxs-lookup"><span data-stu-id="2568f-144">**Design evaluations, advertisement and marketing research:** Eye tracking is a common tool for market research to evaluate website and product designs.</span></span>

### <a name="additional-use-cases"></a><span data-ttu-id="2568f-145">Weitere Anwendungsfälle</span><span class="sxs-lookup"><span data-stu-id="2568f-145">Additional use cases</span></span>
- <span data-ttu-id="2568f-146">**Spiele:** Wollten Sie jemals übernatürliche Kräfte haben?</span><span class="sxs-lookup"><span data-stu-id="2568f-146">**Gaming:** Ever wanted to have superpowers?</span></span> <span data-ttu-id="2568f-147">Hier kommt Ihre Chance!</span><span class="sxs-lookup"><span data-stu-id="2568f-147">Here's your chance!</span></span> <span data-ttu-id="2568f-148">Lassen Sie Hologramme durch Anvisieren frei schweben.</span><span class="sxs-lookup"><span data-stu-id="2568f-148">Levitate holograms by staring at them.</span></span> <span data-ttu-id="2568f-149">Schießen Sie Laserstrahlen aus Ihren Augen.</span><span class="sxs-lookup"><span data-stu-id="2568f-149">Shoot laser beams from your eyes.</span></span> <span data-ttu-id="2568f-150">Verwandeln Sie Feinde in Stein, oder frieren Sie sie ein.</span><span class="sxs-lookup"><span data-stu-id="2568f-150">Turn enemies into stone or freeze them!</span></span> <span data-ttu-id="2568f-151">Verwenden Sie Ihren Röntgenblick, um Gebäude zu erkunden.</span><span class="sxs-lookup"><span data-stu-id="2568f-151">Use your x-ray vision to explore buildings.</span></span> <span data-ttu-id="2568f-152">Ihrer Phantasie sind keine Grenzen gesetzt!</span><span class="sxs-lookup"><span data-stu-id="2568f-152">Your imagination is the limit!</span></span>  

- <span data-ttu-id="2568f-153">**Ausdrucksstarke Avatare:** Die Blickverfolgung ermöglicht ausdrucksvollere 3D-Avatare. Mithilfe der Blickverfolgungsdaten in Echtzeit lassen sich die Augen des Avatars so animieren, dass sie darauf hinweisen, worauf der Benutzer aktuell schaut.</span><span class="sxs-lookup"><span data-stu-id="2568f-153">**Expressive avatars:** Eye tracking aids in more expressive 3D avatars by using live eye tracking date to animate the avatar's eyes to indicate what the user is currently looking at.</span></span> <span data-ttu-id="2568f-154">Durch zusätzliches Zwinkern und Blinzeln wird außerdem mehr Ausdruckskraft erzielt.</span><span class="sxs-lookup"><span data-stu-id="2568f-154">It also adds more expressiveness by adding winks and blinks.</span></span> 

- <span data-ttu-id="2568f-155">**Texteingabe:** Die Blickverfolgung kann als interessante Alternative für eine Texteingabe mit wenig Aufwand verwendet werden, besonders wenn die Benutzung von Sprache oder Händen unpraktisch ist.</span><span class="sxs-lookup"><span data-stu-id="2568f-155">**Text entry:** Eye tracking can be used as an interesting alternative for low-effort text entry especially when speech or hands are inconvenient to use.</span></span> 


## <a name="eye-tracking-api"></a><span data-ttu-id="2568f-156">Blickverfolgungs-API</span><span class="sxs-lookup"><span data-stu-id="2568f-156">Eye tracking API</span></span>
<span data-ttu-id="2568f-157">Bevor wir zu den Einzelheiten der speziellen Entwurfsrichtlinien für die Interaktion durch Anvisieren mit den Augen kommen, möchten wir kurz auf die Funktionen eingehen, über die der Eyetracker von HoloLens 2 verfügt.</span><span class="sxs-lookup"><span data-stu-id="2568f-157">Before going into detail about the specific design guidelines for eye-gaze interaction, we want to briefly point to the capabilities that the HoloLens 2 Eye Tracker is providing.</span></span> <span data-ttu-id="2568f-158">Auf die [Blickverfolgungs-API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) kann über `Windows.Perception.People.EyesPose` zugegriffen werden.</span><span class="sxs-lookup"><span data-stu-id="2568f-158">The [Eye Tracking API](https://docs.microsoft.com/en-us/uwp/api/windows.perception.people.eyespose) is accessible through: `Windows.Perception.People.EyesPose`.</span></span> <span data-ttu-id="2568f-159">Sie stellt Entwicklern einen einzelnen Lichtstrahl zum Anvisieren mit den Augen (Anvisierursprung und -richtung) bereit.</span><span class="sxs-lookup"><span data-stu-id="2568f-159">It provides a single eye gaze ray (gaze origin and direction) to developers.</span></span>
<span data-ttu-id="2568f-160">Der Blickverfolger (Eyetracker) stellt Daten mit einer Bildfrequenz von etwa _30 FPS_ bereit.</span><span class="sxs-lookup"><span data-stu-id="2568f-160">The eye tracker provides data at about _30 FPS_.</span></span>
<span data-ttu-id="2568f-161">Das prognostizierte Anvisieren mit den Augen liegt in einem Sehwinkel von ca.</span><span class="sxs-lookup"><span data-stu-id="2568f-161">The predicted eye gaze lies within ca.</span></span> <span data-ttu-id="2568f-162">1,0-1,5 Grad um das tatsächlich anvisierte Ziel.</span><span class="sxs-lookup"><span data-stu-id="2568f-162">1.0 - 1.5 degrees in visual angle around the actual looked at target.</span></span> <span data-ttu-id="2568f-163">Da geringfügige Ungenauigkeiten zu erwarten sind, sollten Sie einen gewissen Spielraum um diesen unteren Wert einplanen.</span><span class="sxs-lookup"><span data-stu-id="2568f-163">As slight imprecisions are expected, you should plan for some margin around this lower bound value.</span></span> <span data-ttu-id="2568f-164">Dies wird im weiteren Verlauf noch behandelt.</span><span class="sxs-lookup"><span data-stu-id="2568f-164">We will discuss this more below.</span></span> <span data-ttu-id="2568f-165">Damit die Blickverfolgung exakt funktioniert, muss jeder Benutzer eine Benutzerkalibrierung für seine Blickverfolgung durchlaufen.</span><span class="sxs-lookup"><span data-stu-id="2568f-165">For eye tracking to work accurately, each user is required to go through an eye tracking user calibration.</span></span> 

<span data-ttu-id="2568f-166">![Optimale Zielgröße im Abstand von 2 Metern](images/gazetargeting-size-1000px.jpg)</span><span class="sxs-lookup"><span data-stu-id="2568f-166">![Optimal target size at 2 meter distance](images/gazetargeting-size-1000px.jpg)</span></span><br>
<span data-ttu-id="2568f-167">*Optimale Zielgröße im Abstand von 2 Metern*</span><span class="sxs-lookup"><span data-stu-id="2568f-167">*Optimal target size at 2 meter distance*</span></span>


## <a name="eye-gaze-design-guidelines"></a><span data-ttu-id="2568f-168">Entwurfsrichtlinien für das Anvisieren mit den Augen</span><span class="sxs-lookup"><span data-stu-id="2568f-168">Eye gaze design guidelines</span></span>
<span data-ttu-id="2568f-169">Das Erstellen einer Interaktion, welche die schnelle und bewegliche Zieladressierung mit den Augen nutzt, kann eine Herausforderung darstellen.</span><span class="sxs-lookup"><span data-stu-id="2568f-169">Building an interaction that takes advantage of fast moving eye targeting can be challenging.</span></span> <span data-ttu-id="2568f-170">In diesem Abschnitt sind die wichtigsten Vorteile und Herausforderungen zusammengefasst, die Sie beim Entwerfen Ihrer App berücksichtigen sollten.</span><span class="sxs-lookup"><span data-stu-id="2568f-170">In this section, we summarize the key advantages and challenges to take into account when designing your app.</span></span> 

### <a name="benefits-of-eye-gaze-input"></a><span data-ttu-id="2568f-171">Vorteile der Eingabe durch Anvisieren mit den Augen</span><span class="sxs-lookup"><span data-stu-id="2568f-171">Benefits of eye gaze input</span></span>
- <span data-ttu-id="2568f-172">**Zeigen mit hoher Geschwindigkeit.**</span><span class="sxs-lookup"><span data-stu-id="2568f-172">**High speed pointing.**</span></span> <span data-ttu-id="2568f-173">Der Augenmuskel ist der am schnellsten reagierende Muskel in unserem Körper.</span><span class="sxs-lookup"><span data-stu-id="2568f-173">The eye muscle is the fastest reacting muscle in our body.</span></span> 

- <span data-ttu-id="2568f-174">**Wenig Aufwand.**</span><span class="sxs-lookup"><span data-stu-id="2568f-174">**Low effort.**</span></span> <span data-ttu-id="2568f-175">Es sind kaum physische Bewegungen erforderlich.</span><span class="sxs-lookup"><span data-stu-id="2568f-175">Barely any physical movements are necessary.</span></span> 

- <span data-ttu-id="2568f-176">**Selbstverständlichkeit.**</span><span class="sxs-lookup"><span data-stu-id="2568f-176">**Implicitness.**</span></span> <span data-ttu-id="2568f-177">Informationen zu den Blickbewegungen des Benutzers geben dem System Informationen zu dem Ziel, mit dem der Benutzer in Kontakt treten möchte, was häufig von Benutzern als „Gedankenlesen“ bezeichnet wird.</span><span class="sxs-lookup"><span data-stu-id="2568f-177">Often described by users as "mind reading", information about a user's eye movements lets the system know which target the user plans to engage with.</span></span> 

- <span data-ttu-id="2568f-178">**Alternativer Eingabekanal.**</span><span class="sxs-lookup"><span data-stu-id="2568f-178">**Alternative input channel.**</span></span> <span data-ttu-id="2568f-179">Das Anvisieren mit den Augen kann eine leistungsfähige Unterstützung für die Hand- und Spracheingabe darstellen und beruht auf jahrelanger Erfahrung mit der Hand-/Augen-Koordination bei Benutzern.</span><span class="sxs-lookup"><span data-stu-id="2568f-179">Eye gaze can provide a powerful supporting input for hand and voice input building on years of experience from users based on their hand-eye coordination.</span></span>

- <span data-ttu-id="2568f-180">**Visuelle Aufmerksamkeit.**</span><span class="sxs-lookup"><span data-stu-id="2568f-180">**Visual attention.**</span></span> <span data-ttu-id="2568f-181">Ein weiterer wichtiger Vorteil ist die Möglichkeit, abzuleiten, auf was ein Benutzer seine Aufmerksamkeit richtet.</span><span class="sxs-lookup"><span data-stu-id="2568f-181">Another important benefit is the possibility to infer what a user's is paying attention to.</span></span> <span data-ttu-id="2568f-182">Dies kann in verschiedenen Anwendungsbereichen nützlich sein, angefangen bei der effektiveren Auswertung unterschiedlicher Entwürfe über intelligentere Benutzeroberflächen bis hin zu erweiterten sozialen Hinweisen für die Remotekommunikation.</span><span class="sxs-lookup"><span data-stu-id="2568f-182">This can help in various application areas ranging from more effectively evaluating different designs to aiding in smarter User Interfaces and enhanced social cues for remote communication.</span></span>

<span data-ttu-id="2568f-183">Kurz gesagt, die Eingabe durch das Anvisieren mit den Augen bietet potenziell ein schnelles und müheloses kontextbezogenes Signal, das besonders in Kombination mit anderen Eingaben wie *Sprach-* und *Hand*eingaben sehr leistungsstark ist, um die Benutzerabsicht zu bestätigen.</span><span class="sxs-lookup"><span data-stu-id="2568f-183">In a nutshell, using eye gaze as an input potentially offers a fast and effortless contextual signal - This is particularly powerful in combination with other inputs such as *voice* and *manual* input to confirm the user's intent.</span></span>


### <a name="challenges-of-eye-gaze-as-an-input"></a><span data-ttu-id="2568f-184">Herausforderung bei der Eingabe durch Anvisieren mit den Augen</span><span class="sxs-lookup"><span data-stu-id="2568f-184">Challenges of eye gaze as an input</span></span>
<span data-ttu-id="2568f-185">Mit viel Leistungspotenzial geht eine hohe Verantwortung einher: Auch wenn das Anvisieren mit den Augen zum Erstellen magischer Benutzererfahrungen (der Benutzer fühlt sich wie ein Superheld) verwendet werden kann, ist es wichtig zu wissen, wofür es nicht geeignet ist und dies entsprechend zu berücksichtigen.</span><span class="sxs-lookup"><span data-stu-id="2568f-185">With lots of power, comes lots of responsibility: While eye gaze can be used to create magical user experiences feeling like a superhero, it is also important to know what it is not good at to account for this appropriately.</span></span> <span data-ttu-id="2568f-186">Im Folgenden werden einige *Herausforderungen*, die bei der Arbeit mit Eingaben durch Anvisieren mit den Augen auftreten und zu beachten sind, erörtert und Lösungsansätze aufgezeigt:</span><span class="sxs-lookup"><span data-stu-id="2568f-186">In the following, we discuss some *challenges* to take into account and how to address them when working with eye gaze input:</span></span> 

- <span data-ttu-id="2568f-187">**Das Anvisieren mit den Augen ist „immer aktiv“** In dem Moment, in dem Sie das Augenlid öffnen, beginnen Ihre Augen, Dinge in Ihrer Umgebung zu fixieren.</span><span class="sxs-lookup"><span data-stu-id="2568f-187">**Your eye gaze is "always on"** The moment you open your eye lids, your eyes start fixating things in your environment.</span></span> <span data-ttu-id="2568f-188">Wenn auf jeden Ihrer Blicke eine Reaktion erfolgte und Sie möglicherweise versehentlich Aktionen auslösten, da Sie etwas zu lange betrachtet haben, wäre dies eine erschreckende Erfahrung!</span><span class="sxs-lookup"><span data-stu-id="2568f-188">Reacting to every look you make and potentially accidentally issuing actions because you looked at something for too long would result in a terrible experience!</span></span>
<span data-ttu-id="2568f-189">Aus diesem Grund empfehlen wir, das Anvisieren mit den Augen mit einem *Sprachbefehl*, einer *Handgeste*, dem *Anklicken einer Schaltfläche* oder einer verlängerten Verweilzeit zu kombinieren, um die Auswahl eines Ziels auszulösen.</span><span class="sxs-lookup"><span data-stu-id="2568f-189">This is why we recommend combining eye gaze with a *voice command*, *hand gesture*, *button click* or extended dwell to trigger the selection of a target.</span></span>
<span data-ttu-id="2568f-190">Diese Lösung ermöglicht auch einen Modus, in dem der Benutzer frei umherschauen kann, ohne das erdrückende Gefühl haben zu müssen, versehentlich etwas auszulösen.</span><span class="sxs-lookup"><span data-stu-id="2568f-190">This solution also allows for a mode in which the user can freely look around without the overwhelming feeling of involuntarily triggering something.</span></span> <span data-ttu-id="2568f-191">Dieses Problem sollte auch beim Entwerfen von visuellem und akustischem Feedback berücksichtigt werden, wenn der Benutzer lediglich ein Ziel anschaut.</span><span class="sxs-lookup"><span data-stu-id="2568f-191">This issue should also be taken into account when designing visual and auditory feedback when merely looking at a target.</span></span>
<span data-ttu-id="2568f-192">Überfordern Sie den Benutzer nicht mit sofortigen Popup-Effekten oder Sounds beim kurzen Verweilen auf einem Ziel.</span><span class="sxs-lookup"><span data-stu-id="2568f-192">Do not overwhelm the user with immediate pop-out effects or hover sounds.</span></span> <span data-ttu-id="2568f-193">„Dezent“ ist das Mittel der Wahl!</span><span class="sxs-lookup"><span data-stu-id="2568f-193">Subtlety is key!</span></span> <span data-ttu-id="2568f-194">Weiter unten werden im Zusammenhang mit den Entwurfsempfehlungen einige bewährte Methoden hierfür erläutert.</span><span class="sxs-lookup"><span data-stu-id="2568f-194">We will discuss some best practices for this further below when talking about design recommendations.</span></span>

- <span data-ttu-id="2568f-195">**Betrachten und Steuern** Stellen Sie sich vor, Sie möchten ein Bild an der Wand genau ausrichten.</span><span class="sxs-lookup"><span data-stu-id="2568f-195">**Observation vs. control** Imagine you want to precisely align a photograph at your wall.</span></span> <span data-ttu-id="2568f-196">Sie schauen zuerst auf den Rahmen und dann auf die Umgebung, um festzustellen, ob es richtig ausgerichtet ist.</span><span class="sxs-lookup"><span data-stu-id="2568f-196">You look at its borders and its surroundings to see if it aligns well.</span></span> <span data-ttu-id="2568f-197">Stellen Sie sich jetzt vor, wie Sie vorgehen würden, wenn Sie das Anvisieren mit Ihren Augen gleichzeitig als Eingabe zum Verschieben des Bilds verwenden möchten.</span><span class="sxs-lookup"><span data-stu-id="2568f-197">Now imagine how you would do that when at the same time you want to use your eye gaze as an input to move the picture.</span></span> <span data-ttu-id="2568f-198">Schwierig, nicht wahr?</span><span class="sxs-lookup"><span data-stu-id="2568f-198">Difficult, isn't it?</span></span> <span data-ttu-id="2568f-199">Dies beschreibt die doppelte Rolle des Anvisierens mit den Augen, wenn es gleichzeitig für Eingabe und Steuerung verwendet wird.</span><span class="sxs-lookup"><span data-stu-id="2568f-199">This describes the double role of eye gaze when it is required both for input and control.</span></span> 

- <span data-ttu-id="2568f-200">**Abwenden vor dem Klicken:** Untersuchungen haben gezeigt, dass sich bei der schnellen Zielauswahl der Blick des Benutzers möglicherweise bereits vor dem Ausführen eines manuellen Klicks (z.B. Tippen in die Luft) abgewendet hat.</span><span class="sxs-lookup"><span data-stu-id="2568f-200">**Leave before click:** For quick target selections, research has shown that a user's eye gaze may move on before concluding a manual click (e.g., an airtap).</span></span> <span data-ttu-id="2568f-201">Daher ist bei der Synchronisierung des schnellen Blicksignals beim Anvisieren mit den Augen mit der langsameren Steuereingabe (z.B. Sprach- oder Handbefehl, Eingabe über Controller) besondere Aufmerksamkeit geboten.</span><span class="sxs-lookup"><span data-stu-id="2568f-201">Hence, special attention must be paid to synchronizing the fast eye gaze signal with slower control input (e.g., voice, hands, controller).</span></span>

- <span data-ttu-id="2568f-202">**Kleine Ziele:** Kennen Sie das Gefühl, wenn Sie versuchen, einen Text zu lesen, der nur ein wenig zu klein ist, um ihn bequem lesen zu können?</span><span class="sxs-lookup"><span data-stu-id="2568f-202">**Small targets:** Do you know the feeling when you try to read text that is just a bit too small to comfortably read?</span></span> <span data-ttu-id="2568f-203">Dieses Gefühl überstrapazierter Augen, das dazu führt, dass Sie sich ermüdet und gestresst fühlen, weil Sie versuchen, Ihre Augen neu zu justieren, um besser fokussieren zu können?</span><span class="sxs-lookup"><span data-stu-id="2568f-203">This straining feeling on your eyes that cause you to feel tired and worn out because you try to readjust your eyes to focus better?</span></span>
<span data-ttu-id="2568f-204">Dieses Gefühl rufen Sie möglicherweise bei Ihren Benutzern hervor, wenn Sie diese zwingen, bei der Zieladressierung mit den Augen zu kleine Ziele auszuwählen.</span><span class="sxs-lookup"><span data-stu-id="2568f-204">This is a feeling you may invoke in your users when forcing them to select too small targets in your app using eye targeting.</span></span>
<span data-ttu-id="2568f-205">Damit Sie für Ihre Benutzer eine angenehme und komfortable Erfahrung schaffen, sollte bei Zielen für Ihren Entwurf der Sehwinkel mindestens 2 Grad (vorzugsweise mehr) betragen.</span><span class="sxs-lookup"><span data-stu-id="2568f-205">For your design, to create a pleasant and comfortable experience for your users, we recommend that targets should be at least 2° in visual angle, preferably larger.</span></span>

- <span data-ttu-id="2568f-206">**Flatterhafte Blickbewegungen** Unsere Augen vollführen von Fixierung zu Fixierung schnelle Bewegungen.</span><span class="sxs-lookup"><span data-stu-id="2568f-206">**Ragged eye gaze movements** Our eyes perform rapid movements from fixation to fixation.</span></span> <span data-ttu-id="2568f-207">Wenn Sie Scanwege aufgezeichneter Blickbewegungen betrachten, können Sie die Flatterhaftigkeit erkennen.</span><span class="sxs-lookup"><span data-stu-id="2568f-207">If you look at scan paths of recorded eye movements, you can see that they look ragged.</span></span> <span data-ttu-id="2568f-208">Unsere Augen bewegen sich gegenüber dem *Anvisieren mit dem Kopf* oder *Handbewegungen* schnell und springen spontan hin und her.</span><span class="sxs-lookup"><span data-stu-id="2568f-208">Your eyes move quickly and in spontaneous jumps in comparison to *head gaze* or *hand motions*.</span></span>  

- <span data-ttu-id="2568f-209">**Zuverlässigkeit der Verfolgung:** Die Genauigkeit der Blickverfolgung wird ein wenig bei sich ändernden Lichtverhältnissen beeinträchtigt, während sich das Auge an die neuen Bedingungen anpasst.</span><span class="sxs-lookup"><span data-stu-id="2568f-209">**Tracking reliability:** Eye tracking accuracy may degrade a little in changing light as your eye adjust to the new conditions.</span></span>
<span data-ttu-id="2568f-210">Dies wirkt sich jedoch nicht unbedingt auf Ihr App-Design aus, da die Genauigkeit innerhalb der oben beschriebenen Grenze von 2 Grad liegen sollte.</span><span class="sxs-lookup"><span data-stu-id="2568f-210">While this should not necessarily affect your app design, as the accuracy should be within the above mentioned limitation of 2°.</span></span> <span data-ttu-id="2568f-211">Dies kann bedeuten, dass der Benutzer hat eine weitere Kalibrierung ausführen muss.</span><span class="sxs-lookup"><span data-stu-id="2568f-211">It may mean that the user has to run another calibration.</span></span> 


### <a name="design-recommendations"></a><span data-ttu-id="2568f-212">Entwurfsempfehlungen</span><span class="sxs-lookup"><span data-stu-id="2568f-212">Design recommendations</span></span>
<span data-ttu-id="2568f-213">Nachfolgend finden Sie auf Grundlage der beschriebenen Vorteile und Herausforderungen spezielle Entwurfsempfehlungen für die Eingabe durch Anvisieren mit den Augen:</span><span class="sxs-lookup"><span data-stu-id="2568f-213">In the following, we list specific design recommendations based on the described advantages and challenges for eye gaze input:</span></span>

1. <span data-ttu-id="2568f-214">**„Anvisieren mit den Augen“ unterscheidet sich vom „Anvisieren mit dem Kopf“:**</span><span class="sxs-lookup"><span data-stu-id="2568f-214">**Eye gaze != Head gaze:**</span></span>
    - <span data-ttu-id="2568f-215">**Überlegen Sie, ob schnelle, aber gleichzeitig flatterhafte Blickbewegungen zu der Eingabeaufgabe passen:** Während sich schnelle und flatterhafte Blickbewegungen hervorragend für die Auswahl von Zielen im Sichtfeld eignen, sind sie für Aufgaben, die gleichmäßige Eingabeverläufe verlangen (z.B. Zeichnen oder Einkreisen von Bemerkungen) weniger geeignet.</span><span class="sxs-lookup"><span data-stu-id="2568f-215">**Consider whether fast yet ragged eye movements fit your input task:** While our fast and ragged eye movements are great to quickly select targets across our Field of View, it is less applicable for tasks that require smooth input trajectories (e.g., for drawing or encircling annotations).</span></span> <span data-ttu-id="2568f-216">In diesem Fall ist das Zeigen mit Hand oder Kopf zu bevorzugen.</span><span class="sxs-lookup"><span data-stu-id="2568f-216">In this case, hand or head pointing should be preferred.</span></span>
  
    - <span data-ttu-id="2568f-217">**Vermeiden Sie ein direktes Anfügen von Objekten (z. B. Schieberegler oder Cursor) an den Blickverlauf des Benutzers.**</span><span class="sxs-lookup"><span data-stu-id="2568f-217">**Avoid attaching something directly to the user’s eye gaze (e.g., a slider or cursor).**</span></span>
<span data-ttu-id="2568f-218">Bei einem Cursor kann durch den leichten Offset im projizierten Blicksignal der Effekt des „fliehenden Cursors“ entstehen.</span><span class="sxs-lookup"><span data-stu-id="2568f-218">In case of a cursor, this may result in the “fleeing cursor” effect due to slight offsets in the projected eye gaze signal.</span></span> <span data-ttu-id="2568f-219">Bei einem Schieberegler ergeben sich Konflikte mit der Doppelrolle. Sie möchten einerseits den Schieberegler mit Ihren Augen steuern, gleichzeitig aber auch prüfen, ob sich das Objekt an der richtigen Stelle befindet.</span><span class="sxs-lookup"><span data-stu-id="2568f-219">In case of a slider, it conflicts with the double role of controlling the slider with your eyes while also wanting to check whether the object is at the correct location.</span></span> <span data-ttu-id="2568f-220">Kurz gesagt, der Benutzer kann sich schnell überfordert und abgelenkt fühlen, vor allem, wenn das Signal für diesen Benutzer ungenau ist.</span><span class="sxs-lookup"><span data-stu-id="2568f-220">In a nutshell, users may quickly feel overwhelmed and distracted, especially if the signal is imprecise for that user.</span></span> 
  
2. <span data-ttu-id="2568f-221">**Kombinieren Sie das Anvisieren mit den Augen mit anderen Eingaben:** Die Integration der Blickverfolgung mit anderen Eingaben (z.B. Gesten, Sprachbefehle oder Drücken von Schaltflächen) hat mehrere Vorteile:</span><span class="sxs-lookup"><span data-stu-id="2568f-221">**Combine eye gaze with other inputs:** The integration of Eye Tracking with other inputs, such as hand gestures, voice commands or button presses, serves several advantages:</span></span>
    - <span data-ttu-id="2568f-222">**Möglichkeit der freien Betrachtung:** Angesichts der Tatsache, dass die Hauptrolle der Augen die Betrachtung der Umgebung ist, ist es wichtig, dem Benutzer die Möglichkeit zu geben, sich umzuschauen, ohne ein Feedback (visuell, akustisch usw.) oder eine Aktion auszulösen.</span><span class="sxs-lookup"><span data-stu-id="2568f-222">**Allow for free observation:** Given that the main role of our eyes is to observe our environment, it is important to allow users to look around without triggering any (visual, auditory, ...) feedback or actions.</span></span> 
    <span data-ttu-id="2568f-223">Durch die Kombination der Blickverfolgung mit einer anderen Eingabesteuerung ermöglichen Sie einen sanften Übergang zwischen den Blickverfolgungsmodi der freien Betrachtung und der Eingabesteuerung.</span><span class="sxs-lookup"><span data-stu-id="2568f-223">Combining ET with another input control allows for smoothly transitioning between ET observation and input control modes.</span></span>
  
    - <span data-ttu-id="2568f-224">**Leistungsstarker Kontextanbieter:** Mit den Informationen zu den Punkten, auf die der Benutzer bei der Ausgabe eines Sprachbefehls oder der Ausführung einer Handgeste schaut, können Sie die Eingabe mühelos über das Sichtfeld übermitteln.</span><span class="sxs-lookup"><span data-stu-id="2568f-224">**Powerful context provider:** Using information about where the user is looking at while uttering a voice command or performing a hand gesture allows for effortlessly channeling the input across the field-of-view.</span></span> <span data-ttu-id="2568f-225">Beispiele: Mit „Put that there“ (Auswählen, Verschieben, ...hierhin) können Sie schnell und flüssig ein Hologramm auswählen und über die Szene verschieben. Dazu brauchen Sie nur ein Ziel und den Bestimmungsort anzuschauen.</span><span class="sxs-lookup"><span data-stu-id="2568f-225">Examples include: “Put that there” to quickly and fluently select and position a hologram across the scene by simply looking at a target and destination.</span></span> 

    - <span data-ttu-id="2568f-226">**Multimodale Eingaben müssen synchronisiert werden (Problem des vor dem Klicken abgewendeten Blicks):** Die Kombination schneller Blickbewegungen mit komplexeren weiteren Eingaben (z.B. lange Sprachbefehle oder Handgesten) birgt das Risiko, dass der Benutzer seinen Blick abwendet, bevor er den zusätzlichen Eingabebefehl abgeschlossen hat.</span><span class="sxs-lookup"><span data-stu-id="2568f-226">**Need for synchronizing multimodal inputs (“leave before click” issue):** Combining rapid eye movements with more complex additional inputs (e.g., long voice commands or hand gestures) bears the risk of moving on with your eye gaze before finishing the additional input command.</span></span> <span data-ttu-id="2568f-227">Daher sollten Sie beim Erstellen eigener Eingabesteuerungen (z.B. benutzerdefinierte Handgesten) unbedingt das Einsetzen dieser Eingabe und die ungefähre Dauer protokollieren, um dies auf die Fixierdauer abzustimmen, die Benutzer in der Vergangenheit gezeigt haben.</span><span class="sxs-lookup"><span data-stu-id="2568f-227">Hence, if you create your own input controls (e.g., custom hand gestures), make sure to log the onset of this input or approximate duration to correlate it with what a user had fixated on in the past.</span></span>
    
3. <span data-ttu-id="2568f-228">**Dezentes Feedback für Eingaben über die Blickverfolgung:** Es ist zwar nützlich, ein Feedback bereitzustellen, wenn ein Ziel anvisiert wird (um anzugeben, dass das System erwartungsgemäß funktioniert), jedoch sollte es dezent gehalten werden.</span><span class="sxs-lookup"><span data-stu-id="2568f-228">**Subtle feedback for eye tracking input:** It is useful to provide feedback if a target is looked at (to indicate that the system is working as intended) but should be kept subtle.</span></span> <span data-ttu-id="2568f-229">Dazu zählen möglicherweise das langsame Ein- und Ausblenden von visuellen Markierungen oder das fast unmerkliche Verhalten anderer Ziele wie in Zeitlupe (z.B. leichtes Vergrößern des Ziels), um anzugeben, dass das System korrekt das Anvisieren eines Ziels durch den Benutzer erkannt hat, jedoch ohne unnötigerweise den aktuellen Workflow des Benutzers zu unterbrechen.</span><span class="sxs-lookup"><span data-stu-id="2568f-229">This may include slowly blending in/out visual highlights or perform other subtle target behaviors, such as slow motions (e.g., slightly increasing the target) to indicate that the system correctly detected that the user is looking at a target, however, without unnecessarily interrupting the user’s current workflow.</span></span> 

4. <span data-ttu-id="2568f-230">**Vermeiden des Erzwingens unnatürlicher Augenbewegungen für die Eingabe:** Zwingen Sie Ihre Benutzer nicht, zum Auslösen von Aktionen in Ihrer App bestimmte Augenbewegungen (Fixierbewegungen) auszuführen.</span><span class="sxs-lookup"><span data-stu-id="2568f-230">**Avoid enforcing unnatural eye movements as input:** Do not force users to perform specific eye movements (gaze gestures) to trigger actions in your app.</span></span>

5. <span data-ttu-id="2568f-231">**Berücksichtigen von Ungenauigkeiten:** Wir unterscheiden zwei Arten von Ungenauigkeiten, die für den Benutzer wahrnehmbar sind: Offset und Jitter.</span><span class="sxs-lookup"><span data-stu-id="2568f-231">**Account for imprecisions:** We distinguish two types of imprecisions which are noticeable to users: Offset and Jitter.</span></span> <span data-ttu-id="2568f-232">Die einfachste Möglichkeit zur Behandlung von Offsets ist die Bereitstellung ausreichend großer Ziele für die Interaktion (Sehwinkel >2 ° – Referenz: Ihr Daumennagel befindet sich in einem Sehwinkel von ca. 2 °, wenn Sie den Arm ausstrecken(1)).</span><span class="sxs-lookup"><span data-stu-id="2568f-232">The easiest way to address offsets is to provide sufficiently large targets to interact with (> 2° in visual angle – as reference: your thumbnail is about 2° in visual angle when you stretch out your arm (1)).</span></span> <span data-ttu-id="2568f-233">Dies führt zu folgender Richtlinie:</span><span class="sxs-lookup"><span data-stu-id="2568f-233">This leads to the following guidance:</span></span>
    - <span data-ttu-id="2568f-234">Zwingen Sie Benutzer nicht, kleine Ziele auszuwählen: Untersuchung haben ergeben, dass Benutzer die Interaktion als mühelos und zauberhaft beschreiben, wenn Ziele groß genug sind (und auch das System gut entworfen ist).</span><span class="sxs-lookup"><span data-stu-id="2568f-234">Do not force users to select tiny targets: Research has shown that if targets are sufficiently large (and the system is designed well), users describe the interaction as effortless and magical.</span></span> <span data-ttu-id="2568f-235">Wenn Ziele zu klein sind, empfinden Benutzer die Erfahrung als ermüdend und frustrierend.</span><span class="sxs-lookup"><span data-stu-id="2568f-235">If targets become too small, users describe the experience as fatiguing and frustrating.</span></span>
   

## <a name="see-also"></a><span data-ttu-id="2568f-236">Weitere Informationen</span><span class="sxs-lookup"><span data-stu-id="2568f-236">See also</span></span>
* [<span data-ttu-id="2568f-237">Anvisieren mit dem Kopf und Ausführen</span><span class="sxs-lookup"><span data-stu-id="2568f-237">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="2568f-238">Anvisieren mit dem Kopf und mit den Augen in DirectX</span><span class="sxs-lookup"><span data-stu-id="2568f-238">Head and eye gaze in DirectX</span></span>](gaze-in-directx.md)
* [<span data-ttu-id="2568f-239">Anvisieren mit den Augen in Unity (Mixed Reality-Toolkit)</span><span class="sxs-lookup"><span data-stu-id="2568f-239">Eye gaze in Unity (Mixed Reality Toolkit)</span></span>](https://aka.ms/mrtk-eyes)
* [<span data-ttu-id="2568f-240">Handgesten</span><span class="sxs-lookup"><span data-stu-id="2568f-240">Hand gestures</span></span>](gestures.md)
* [<span data-ttu-id="2568f-241">Spracheingabe</span><span class="sxs-lookup"><span data-stu-id="2568f-241">Voice input</span></span>](voice-design.md)
* [<span data-ttu-id="2568f-242">Motion-Controller</span><span class="sxs-lookup"><span data-stu-id="2568f-242">Motion controllers</span></span>](motion-controllers.md)
* [<span data-ttu-id="2568f-243">Komfort</span><span class="sxs-lookup"><span data-stu-id="2568f-243">Comfort</span></span>](comfort.md)
