---
title: Zeigen und Ausführen mit den Händen
description: Übersicht über das Eingabemodell „Zeigen und Ausführen“
author: caseymeekhof
ms.author: cmeekhof
ms.date: 04/05/2019
ms.topic: article
ms.localizationpriority: high
keywords: Mixed Reality, Interaktion, Design, HoloLens, Hände, fern, Zeigen und Ausführen
ms.openlocfilehash: 30f85d2bb455abab3a533e0a829b4fba8cea0a7a
ms.sourcegitcommit: f20beea6a539d04e1d1fc98116f7601137eebebe
ms.translationtype: HT
ms.contentlocale: de-DE
ms.lasthandoff: 06/05/2019
ms.locfileid: "66402384"
---
# <a name="point-and-commit-with-hands"></a><span data-ttu-id="5126b-104">Zeigen und Ausführen mit den Händen</span><span class="sxs-lookup"><span data-stu-id="5126b-104">Point and commit with hands</span></span>
<span data-ttu-id="5126b-105">„Zeigen und Ausführen mit den Händen“ ist ein Eingabemodell, mit dem Benutzer aus der Entfernung auf 2D-Inhalte und 3D-Objekte zielen, diese auswählen und bearbeiten können.</span><span class="sxs-lookup"><span data-stu-id="5126b-105">Point and commit with hands is an input model that enables users to target, select and manipulate 2D content and 3D objects in the distance.</span></span> <span data-ttu-id="5126b-106">Diese „ferne“ Interaktionstechnik gibt es nur in der Mixed Reality-Umgebung; sie entspricht nicht der Art und Weise, in der Menschen normalerweise mit der realen Welt in Interaktion treten.</span><span class="sxs-lookup"><span data-stu-id="5126b-106">This "far" interaction technique is unique to mixed reality and is not a way humans naturally intereact with the real world.</span></span> <span data-ttu-id="5126b-107">Im Superhelden-Film *X-Men* beispielsweise kann die Figur [Magneto](https://en.wikipedia.org/wiki/Magneto_(comics)) mit seinen Händen aus der Entfernung nach einem fernen Objekt greifen und dieses manipulieren.</span><span class="sxs-lookup"><span data-stu-id="5126b-107">For example, in the super hero movie *X-Men*, the character [Magneto](https://en.wikipedia.org/wiki/Magneto_(comics)) is capable of reaching out and manipulating a far object in the distance with his hands.</span></span> <span data-ttu-id="5126b-108">Dies ist etwas, was Menschen in der Realität nicht können.</span><span class="sxs-lookup"><span data-stu-id="5126b-108">This is not something humans can do in reality.</span></span> <span data-ttu-id="5126b-109">In HoloLens (AR) und Mixed Reality (VR) statten wir Benutzer mit diesen magischen Kräften aus und setzen dabei die Grenzen der Physik in der realen Welt außer Kraft. Ziel ist nicht nur ein positives Erlebnis mit holografischen Inhalten, sondern auch eine effektivere und effizientere Interaktion.</span><span class="sxs-lookup"><span data-stu-id="5126b-109">In both HoloLens (AR) and Mixed Reality (VR), we equip users with this magical power, breaking the physical constraint of the real world not only to have a delightful experience with holographic contents but also to make the interaction more effective and efficient.</span></span>

## <a name="device-support"></a><span data-ttu-id="5126b-110">Unterstützung von Geräten</span><span class="sxs-lookup"><span data-stu-id="5126b-110">Device support</span></span>

<span data-ttu-id="5126b-111">Eingabemodell</span><span class="sxs-lookup"><span data-stu-id="5126b-111">Input model</span></span> | [<span data-ttu-id="5126b-112">HoloLens (1. Generation)</span><span class="sxs-lookup"><span data-stu-id="5126b-112">HoloLens (1st gen)</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/hololens-hardware-details) | <span data-ttu-id="5126b-113">HoloLens 2</span><span class="sxs-lookup"><span data-stu-id="5126b-113">HoloLens 2</span></span> | [<span data-ttu-id="5126b-114">Immersive Headsets</span><span class="sxs-lookup"><span data-stu-id="5126b-114">Immersive headsets</span></span>](https://docs.microsoft.com/en-us/windows/mixed-reality/immersive-headset-hardware-details) |
| ---------| -----| ----- | ---------|
<span data-ttu-id="5126b-115">Zeigen und Ausführen mit den Händen</span><span class="sxs-lookup"><span data-stu-id="5126b-115">Point and commit with hands</span></span> | <span data-ttu-id="5126b-116">❌ Nicht unterstützt</span><span class="sxs-lookup"><span data-stu-id="5126b-116">❌ Not supported</span></span> | <span data-ttu-id="5126b-117">✔️ Empfohlen</span><span class="sxs-lookup"><span data-stu-id="5126b-117">✔️ Recommended</span></span> | <span data-ttu-id="5126b-118">✔️ Empfohlen</span><span class="sxs-lookup"><span data-stu-id="5126b-118">✔️ Recommended</span></span>

<span data-ttu-id="5126b-119">„Zeigen und Ausführen“, auch bezeichnet als „Hands Far“ (Hände weit entfernt), ist eines der neuen Features, welches das neue bewegliche Handverfolgungssystem verwendet.</span><span class="sxs-lookup"><span data-stu-id="5126b-119">Point and commit, also known as hands far, is one of the new features that utilizes the new articulated hand-tracking system.</span></span> <span data-ttu-id="5126b-120">Dieses Eingabemodell ist auch das primäre Eingabemodell bei immersiven Headsets und Verwendung von Motion-Controllern.</span><span class="sxs-lookup"><span data-stu-id="5126b-120">This input model is also the primary input model on immersive headsets through the use of motion controllers.</span></span>

## <a name="hand-rays"></a><span data-ttu-id="5126b-121">Handlichtstrahl</span><span class="sxs-lookup"><span data-stu-id="5126b-121">Hand rays</span></span>

<span data-ttu-id="5126b-122">In HoloLens 2 haben wir einen Lichtstrahl erzeugt, der aus der Mitte der Handfläche schießt.</span><span class="sxs-lookup"><span data-stu-id="5126b-122">On HoloLens 2, we created a hand ray that shoots out from the center of a palm.</span></span> <span data-ttu-id="5126b-123">Dieser Lichtstrahl wird als Erweiterung der Hand behandelt.</span><span class="sxs-lookup"><span data-stu-id="5126b-123">This ray is treated as an extension of the hand.</span></span> <span data-ttu-id="5126b-124">Ein ringförmiger Cursor befindet sich am Ende des Strahls, um den Punkt anzugeben, an dem der Strahl das Zielobjekt schneidet.</span><span class="sxs-lookup"><span data-stu-id="5126b-124">A donut-shaped cursor is attached to the end of the ray to indicate the location where the ray intersects with a target object.</span></span> <span data-ttu-id="5126b-125">Das Objekt, auf dem der Cursor landet, kann dann gestische Befehle von der Hand empfangen.</span><span class="sxs-lookup"><span data-stu-id="5126b-125">The object that the cursor lands on can then receive gestural commands from the hand.</span></span>

<span data-ttu-id="5126b-126">Dieser grundlegende gestische Befehl wird mit dem Daumen und dem Zeigefinger ausgelöst, um das Tippen in die Luft auszuführen.</span><span class="sxs-lookup"><span data-stu-id="5126b-126">This basic gestural command is triggered by using the thumb and index finger to perform the air-tap action.</span></span> <span data-ttu-id="5126b-127">Mithilfe des Handlichtstrahls zum Zeigen und dem Tippen in die Luft zum Ausführen kann der Benutzer eine Schaltfläche oder einen Link für Webinhalte aktivieren.</span><span class="sxs-lookup"><span data-stu-id="5126b-127">By using the hand ray to point and air tap to commit, users can activate a button or a hyperlink on a web content.</span></span> <span data-ttu-id="5126b-128">Mit weiteren zusammengesetzten Gesten kann der Benutzer durch Webinhalte navigieren und 3D-Objekte aus der Entfernung bearbeiten.</span><span class="sxs-lookup"><span data-stu-id="5126b-128">With more composite gestures, users are capable of navigating web content and manipulating 3D objects from a distance.</span></span> <span data-ttu-id="5126b-129">Das visuelle Design des Handlichtstrahls sollte auch auf die Zustände beim Zeigen und Ausführen reagieren, wie nachstehend beschrieben und gezeigt:</span><span class="sxs-lookup"><span data-stu-id="5126b-129">The visual design of the hand ray should also react to these point and commit states, as described and shown below:</span></span> 

* <span data-ttu-id="5126b-130">Im Status *Zeigen* ist der Lichtstrahl eine gestrichelte Linie, und der Cursor hat die Form eines Rings.</span><span class="sxs-lookup"><span data-stu-id="5126b-130">In the *pointing* state, the ray is a dash line and the cursor is a donut shape.</span></span>
* <span data-ttu-id="5126b-131">Im Status *Ausführen* wird der Lichtstrahl zu einer durchgezogenen Linie, und der Cursor wird auf einen Punkt verkleinert.</span><span class="sxs-lookup"><span data-stu-id="5126b-131">In the *commit* state, the ray turns into a solid line and the cursor shrinks to a dot.</span></span>

![](images/Hand-Rays-720px.jpg)

## <a name="transition-between-near-and-far"></a><span data-ttu-id="5126b-132">Übergang zwischen nah und fern</span><span class="sxs-lookup"><span data-stu-id="5126b-132">Transition between near and far</span></span>

<span data-ttu-id="5126b-133">Statt bestimmte Gesten zu verwenden (z. B. „Zeigen mit dem Zeigefinger" zum Richten des Lichtstrahls), haben wir den Lichtstrahl so entworfen, dass er aus der Mitte der Handfläche kommt, wodurch die fünf Finger frei gehalten und für manipulativere Gesten (z. B. Finger zusammenführen und greifen) reserviert werden konnten.</span><span class="sxs-lookup"><span data-stu-id="5126b-133">Instead of using specific gesture, such as "pointing with index finger" to direct the ray, we designed the ray coming out from the center of the palm, releasing and reserving the five fingers for more manipulative gestures, such as pinch and grab.</span></span> <span data-ttu-id="5126b-134">Mit diesem Entwurf erstellen wir nur ein mentales Modell, das exakt den gleichen Satz von Gesten für nahe und ferne Interaktionen unterstützt.</span><span class="sxs-lookup"><span data-stu-id="5126b-134">With this design, we create only one mental model, supporting exactly the same set of hand gestures for both near and far interaction.</span></span> <span data-ttu-id="5126b-135">So können Sie die gleiche Greifgeste zum Bearbeiten von Objekten aus unterschiedlichen Entfernungen verwenden.</span><span class="sxs-lookup"><span data-stu-id="5126b-135">You can use the same grab gesture to manipulate objects at different distances.</span></span> <span data-ttu-id="5126b-136">Der Aufruf des Lichtstrahls erfolgt automatisch und basiert auf der Entfernung:</span><span class="sxs-lookup"><span data-stu-id="5126b-136">The invocation of the rays is automatic and proximity based:</span></span>

*  <span data-ttu-id="5126b-137">Wenn sich ein Objekt in Armeslänge befindet (ungefähr 50 cm), wird der Lichtstrahl automatisch deaktiviert und regt so die Interaktion aus der Nähe an.</span><span class="sxs-lookup"><span data-stu-id="5126b-137">When an object is within arm reached distance (roughly 50 cm), the rays are turned off automatically encouraging for near interaction.</span></span>
*  <span data-ttu-id="5126b-138">Wenn das Objekt mehr als 50 cm entfernt ist, wird der Lichtstrahl aktiviert.</span><span class="sxs-lookup"><span data-stu-id="5126b-138">When the object is farther than 50 cm, the rays are turned on.</span></span> <span data-ttu-id="5126b-139">Der Übergang sollte reibungslos und nahtlos erfolgen.</span><span class="sxs-lookup"><span data-stu-id="5126b-139">The transition should be smooth and seamless.</span></span>

![](images/Transition-Between-Near-And-Far-720px.jpg)

## <a name="2d-slate-interaction"></a><span data-ttu-id="5126b-140">2D-Tafel – Interaktion</span><span class="sxs-lookup"><span data-stu-id="5126b-140">2D slate interaction</span></span>

<span data-ttu-id="5126b-141">Eine 2D-Tafel ist ein holografischer Container, der 2D-App-Inhalte hostet (z. B. Webbrowser).</span><span class="sxs-lookup"><span data-stu-id="5126b-141">A 2D Slate is a holographic container hosting 2D app contents, such as web browser.</span></span> <span data-ttu-id="5126b-142">Das Entwurfskonzept für die ferne Interaktion mit einer 2D-Tafel verwendet den Handlichtstrahl zum Zielen und das Tippen in die Luft zum Auswählen.</span><span class="sxs-lookup"><span data-stu-id="5126b-142">The design concept for far interacting with a 2D slate is to use hand rays to target and air tap to select.</span></span> <span data-ttu-id="5126b-143">Nach dem Zielen mit dem Handlichtstrahl kann der Benutzer in die Luft tippen, um einen Link oder eine Schaltfläche auszulösen.</span><span class="sxs-lookup"><span data-stu-id="5126b-143">After targeting with a hand ray, users can air tap to trigger a hyperlink or a button.</span></span> <span data-ttu-id="5126b-144">Sie können eine Hand zum „Tippen in die Luft und Ziehen“ verwenden, um den Inhalt einer Tafel nach oben und unten zu scrollen.</span><span class="sxs-lookup"><span data-stu-id="5126b-144">They can use one hand to "air tap and drag" to scroll a slate content up and down.</span></span> <span data-ttu-id="5126b-145">Mit der relativen Bewegung und der Verwendung von zwei Händen zum „Tippen in die Luft und Ziehen“ können Sie den Inhalt der Tafel vergrößern und verkleinern.</span><span class="sxs-lookup"><span data-stu-id="5126b-145">The relative motion of using two hands to air tap and drag can zoom in and out the slate content.</span></span>

<span data-ttu-id="5126b-146">Wenn Sie mit dem Handlichtstrahl in die Ecken und auf die Kanten zielen, wird das naheliegendste Bearbeitungsangebot angezeigt.</span><span class="sxs-lookup"><span data-stu-id="5126b-146">Targeting the hand ray at the corners and edges reveals the closest manipulation affordance.</span></span> <span data-ttu-id="5126b-147">Mit den Bearbeitungsangeboten „Greifen und Ziehen“ kann der Benutzer über das Eckenangebot eine einheitliche Skalierung ausführen und über das Kantenangebot die Tafel neu formatieren.</span><span class="sxs-lookup"><span data-stu-id="5126b-147">By "grab and drag" the manipulation affordances, users can perform uniform scaling through the corner affordances and can reflow the slate via the edge affordances.</span></span> <span data-ttu-id="5126b-148">Durch Greifen und Ziehen der Hololeiste im oberen Bereich der 2D-Tafel kann der Benutzer die gesamte Tafel verschieben.</span><span class="sxs-lookup"><span data-stu-id="5126b-148">Grabbing and dragging the holobar at the top of the 2D slate can users move the whole slate.</span></span>

![](images/2D-Slate-Interaction-Far-720px.jpg)

<span data-ttu-id="5126b-149">So bearbeiten Sie die eigentliche 2D-Tafel</span><span class="sxs-lookup"><span data-stu-id="5126b-149">For manipulating the 2D slate itself:</span></span><br>

* <span data-ttu-id="5126b-150">Der Benutzer zeigt mit dem Handlichtstrahl in die Ecken oder auf die Kanten, um das naheliegendste Bearbeitungsangebot anzuzeigen.</span><span class="sxs-lookup"><span data-stu-id="5126b-150">Users point the hand ray at the corners or edges to reveal the closest manipulation affordance.</span></span> 
* <span data-ttu-id="5126b-151">Durch das Anwenden einer Bearbeitungsgeste kann der Benutzer über das Eckenangebot eine einheitliche Skalierung und über das Kantenangebot eine Formatierung der Tafel vornehmen.</span><span class="sxs-lookup"><span data-stu-id="5126b-151">By applying a manipulation gesture on the affordance, users can perform uniform scaling through the corner affordance and can reflow the slate via the edge affordance.</span></span> 
* <span data-ttu-id="5126b-152">Durch das Anwenden einer Bearbeitungsgeste auf die Hololeiste im oberen Bereich der 2D-Tafel kann der Benutzer die gesamte Tafel verschieben.</span><span class="sxs-lookup"><span data-stu-id="5126b-152">By applying a manipulation gesture on the holobar at the top of the 2D slate, users can move the whole slate.</span></span><br>

<br>

## <a name="3d-object-manipulation"></a><span data-ttu-id="5126b-153">3D-Objektbearbeitung</span><span class="sxs-lookup"><span data-stu-id="5126b-153">3D object manipulation</span></span>

<span data-ttu-id="5126b-154">Bei der direkten Bearbeitung hat der Benutzer zwei Möglichkeiten zum Bearbeiten eines 3D-Objekts: angebotsbasierte Bearbeitung und Bearbeitung ohne Angebot.</span><span class="sxs-lookup"><span data-stu-id="5126b-154">In direct manipulation, there are two ways for users to manipulate 3D object, affordance-based manipulation and non-affordance based manipulation.</span></span> <span data-ttu-id="5126b-155">Beim Modell „Zeigen und Ausführen“ kann der Benutzer genau die gleichen Aufgaben über den Handlichtstrahl ausführen.</span><span class="sxs-lookup"><span data-stu-id="5126b-155">In the point and commit model, users are capable of achieving exactly the same tasks through the hand rays.</span></span> <span data-ttu-id="5126b-156">Es ist keine zusätzliche Schulung erforderlich.</span><span class="sxs-lookup"><span data-stu-id="5126b-156">No additional learning is needed.</span></span><br>

### <a name="affordance-based-manipulation"></a><span data-ttu-id="5126b-157">Angebotsbasierte Bearbeitung</span><span class="sxs-lookup"><span data-stu-id="5126b-157">Affordance-based manipulation</span></span>
<span data-ttu-id="5126b-158">Der Benutzer verwendet den Handlichtstrahl zum Zeigen und Anzeigen des Begrenzungsrahmens und der Bearbeitungsangebote.</span><span class="sxs-lookup"><span data-stu-id="5126b-158">Users use hand rays to point and reveal the bounding box and manipulation affordances.</span></span> <span data-ttu-id="5126b-159">Der Benutzer kann die Bearbeitungsgeste auf den Begrenzungsrahmen anwenden, um das gesamte Objekt zu verschieben, auf die Kantenangebote, um das Objekt zu drehen, und auf die Eckenangebote, um eine einheitliche Skalierung vorzunehmen.</span><span class="sxs-lookup"><span data-stu-id="5126b-159">Users can apply the manipulation gesture on the bounding box to move the whole object, on the edge affordances to rotate and on the coner affordances to scale uniformly.</span></span> <br>

![](images/3D-Object-Manipulation-Far-720px.jpg) <br>


### <a name="non-affordance-based-manipulation"></a><span data-ttu-id="5126b-160">Bearbeitung ohne Angebot</span><span class="sxs-lookup"><span data-stu-id="5126b-160">Non-affordance based manipulation</span></span>
<span data-ttu-id="5126b-161">Der Benutzer zeigt mit dem Handlichtstrahl, um den Begrenzungsrahmen anzuzeigen, und wendet dann direkt Bearbeitungsgesten darauf an.</span><span class="sxs-lookup"><span data-stu-id="5126b-161">Users point with hand rays to reveal the bounding box then directly apply manipulation gestures on it.</span></span> <span data-ttu-id="5126b-162">Mit einer Hand sind Verschiebung und Drehung des Objekts der Bewegung und Ausrichtung der Hand zugeordnet.</span><span class="sxs-lookup"><span data-stu-id="5126b-162">With one hand, the translation and rotation of the object are associated to motion and orientation of the hand.</span></span> <span data-ttu-id="5126b-163">Mit zwei Händen kann der Benutzer Verschiebung, Drehung und Skalierung entsprechend der relativen Bewegung der beiden Hände vornehmen.</span><span class="sxs-lookup"><span data-stu-id="5126b-163">With two hands, users can translate, scale and rotate it according to relative motions of two hands.</span></span><br>

<br>

## <a name="instinctual-gesturers"></a><span data-ttu-id="5126b-164">Instinktive Gesten</span><span class="sxs-lookup"><span data-stu-id="5126b-164">Instinctual gesturers</span></span>
<span data-ttu-id="5126b-165">Das Konzept der instinktiven Gesten für „Zeigen und Ausführen“ entspricht dem für die direkte Bearbeitung.</span><span class="sxs-lookup"><span data-stu-id="5126b-165">The concept of instinctual gestures for point and commit is similar to that for direct manipulation.</span></span> <span data-ttu-id="5126b-166">Die Gesten, die der Benutzer bei einem 3D-Objekt ausführen soll, werden über das Design der Benutzeroberflächenangebote gesteuert.</span><span class="sxs-lookup"><span data-stu-id="5126b-166">The gestures users are suppose to perform on a 3D object are guided by the design of UI affordances.</span></span> <span data-ttu-id="5126b-167">Ein kleiner Steuerpunkt beispielsweise kann den Benutzer motivieren, Daumen und Zeigefinger zusammenzuführen, während ein anderer Benutzer möglicherweise ein größeres Objekt mit allen 5 Fingern greifen möchte.</span><span class="sxs-lookup"><span data-stu-id="5126b-167">For example, a small control point might motivate users to pinch with their thumb and index finger, while a user might want to grab a larger object using all 5 fingers.</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)<br>

## <a name="symmetric-design-between-hands-and-6-dof-controller"></a><span data-ttu-id="5126b-168">Symmetrisches Design für Händen und Controller mit 6 Freiheitsgraden</span><span class="sxs-lookup"><span data-stu-id="5126b-168">Symmetric design between hands and 6 DoF controller</span></span> 
<span data-ttu-id="5126b-169">Das Konzept „Zeigen und Ausführen“ für die ferne Interaktion wurde anfänglich für das Mixed Reality-Portal (MRP) erstellt, in dem ein Benutzer ein immersives Headset trägt und über Motion-Controller mit 3D-Objekten interagiert.</span><span class="sxs-lookup"><span data-stu-id="5126b-169">The concept of point and commit for far interaction was initially created and defined for the Mixed Reality Portal (MRP), where a user wears an immersive headset and interacts with 3D objects via motion controllers.</span></span> <span data-ttu-id="5126b-170">Zum Zeigen auf und Bearbeiten von fernen Objekten schießen die Motion-Controller einen Lichtstrahl ab.</span><span class="sxs-lookup"><span data-stu-id="5126b-170">The motion controllers shoot out rays for pointing and manipulating far objects.</span></span> <span data-ttu-id="5126b-171">zum Ausführen weiterer unterschiedlicher Aktionen gibt es Tasten auf den Controllern.</span><span class="sxs-lookup"><span data-stu-id="5126b-171">There are buttons on the controllers for further committing different actions.</span></span> <span data-ttu-id="5126b-172">Wir nutzen das Interaktionsmodell des Lichtstrahls und haben dieses mit den beiden Händen verbunden.</span><span class="sxs-lookup"><span data-stu-id="5126b-172">We leverage the interaction model of rays and attached them to both hands.</span></span> <span data-ttu-id="5126b-173">Bei diesem symmetrischen Entwurf braucht der Benutzer, der sich mit MRP auskennt, kein anderes Interaktionsmodell für das Zeigen auf und Bearbeiten von fernen Objekten zu lernen, wenn er HoloLen 2 verwendet (und umgekehrt).</span><span class="sxs-lookup"><span data-stu-id="5126b-173">With this symmetric design, users who are familiar with MRP won't need to learn another interaction model for far pointing and manipulation when they use HoloLen 2, and vice versa.</span></span>    

![](images/Symmetric-Design-For-Rays-720px.jpg)<br>

## <a name="instinctual-gestures"></a><span data-ttu-id="5126b-174">Instinktive Gesten</span><span class="sxs-lookup"><span data-stu-id="5126b-174">Instinctual gestures</span></span>

![](images/Instinctual-Gestures-Far-720px.jpg)

## <a name="see-also"></a><span data-ttu-id="5126b-175">Weitere Informationen</span><span class="sxs-lookup"><span data-stu-id="5126b-175">See also</span></span>
* [<span data-ttu-id="5126b-176">Anvisieren mit dem Kopf und Ausführen</span><span class="sxs-lookup"><span data-stu-id="5126b-176">Head-gaze and commit</span></span>](gaze-and-commit.md)
* [<span data-ttu-id="5126b-177">Direkte Manipulation mit den Händen</span><span class="sxs-lookup"><span data-stu-id="5126b-177">Direct manipulation with hands</span></span>](direct-manipulation.md)
* [<span data-ttu-id="5126b-178">Instinktive Interaktionen</span><span class="sxs-lookup"><span data-stu-id="5126b-178">Instinctual interactions</span></span>](interaction-fundamentals.md)

