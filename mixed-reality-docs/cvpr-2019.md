---
title: Maschinelles Sehen Anwendungen für den Mixed Reality-Headsets-Workshop bei CVPR 2019
description: Übersicht und Zeitplan des Maschinelles Sehen Anwendungen für den Remix-Workshop für gemischte Realität, die auf der CVPR-Konferenz am 2019. Juni geliefert werden.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: Event, Research Mode, CVPR, Maschinelles sehen, Forschung, hololens
ms.openlocfilehash: 89d79bcef77043564e51faada940d2c71a6005e4
ms.sourcegitcommit: 2f600e5ad00cd447b180b0f89192b4b9d86bbc7e
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 06/15/2019
ms.locfileid: "67148709"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a>Maschinelles Sehen von Anwendungen für Mixed Reality-Headsets

Organisiert in Verbindung mit [CVPR 2019](http://cvpr2019.thecvf.com/)

Long-Strand (ca)

17. Juni 2019 (Mittag)-Hyatt-Regency F


## <a name="organizers"></a>Organis
* Marc Pollefeys
* Federica Bogo
* Johannes Schönberger
* Osman Ulusoy

## <a name="overview"></a>Übersicht

![Teaser-Bild](images/cvpr2019_teaser2.jpg)

Gemischte Reality-Headsets, wie z. b. Microsoft hololens, werden zu leistungsfähigen Plattformen, um maschinelles sehen-Anwendungen Der hololens Research-Modus ermöglicht Maschinelles sehen auf Geräten durch die Bereitstellung von Zugriff auf alle Rohbild-Sensordaten Ströme, einschließlich der Tiefe und der IR. Da der Research-Modus jetzt seit dem 2018 von Mai verfügbar ist, beginnen wir mit der Entwicklung einiger interessanter Demos und Anwendungen, die für hololens entwickelt werden. 

Das Ziel dieses Workshops besteht darin, Studenten und Forscher zusammenzubringen, die an der Maschinelles sehen für gemischte Reality-Anwendungen interessiert sind. Der Workshop stellt einen Veranstaltungsort für die gemeinsame Nutzung von Demos und Anwendungen bereit und erfährt von einander, um Anwendungen zu entwickeln oder zu portieren. 

Wir empfehlen Übermittlungen zu den Themen der (egozentrischen) Objekterkennung, Hand-und Benutzer Nachverfolgung, Aktivitäts Erkennung, Slam, 3D-Rekonstruktion, Szenen Verständnis, sensorbasierte Lokalisierung, Navigation und mehr.

## <a name="paper-submission"></a>Papier Übermittlung
* Stichtag für Papier Übermittlung: 17. Mai
* Benachrichtigung an Autoren: 24. Mai

Für Papier Übermittlungen sollte die CVPR-Vorlage verwendet werden, und Sie sind auf vier Seiten plus Verweise beschränkt. Außerdem empfehlen wir den Autoren, ein Video zu übermitteln, in dem Ihre Anwendung vorgestellt wird.
Beachten Sie, dass Übermittlungen bereits veröffentlichter Arbeit zulässig sind (einschließlich der Arbeit, die an die Hauptkonferenz CVPR 2019 akzeptiert wird). 

Einsendungen können auf das CMT hochgeladen werden: https://cmt3.research.microsoft.com/CVFORMR2019

Im Workshop wird eine Teilmenge der Dokumente zur mündlichen Präsentation ausgewählt. Wir empfehlen jedoch allen Autoren dringend, ihre Arbeit während der Demo Sitzung zu präsentieren.


## <a name="schedule"></a>Zeitplan
* 13:30-13:45: Begrüßungs-und Öffnungs Hinweise.
* 13:45-14:15: **Keynote Talk**: Prof. Marc Pollefeys, ETH Zürich/Microsoft. Tel Egozentrische Maschinelles sehen auf hololens.
* 14:15-14:45: **Keynote Talk**: Prof. Kris Kitani, Carnegie Mellon University. Tel Die egozentrische Aktivität und das darstellen von Vorhersagen.
* 14:45-15:15: **Keynote Talk**: Herr. Yang Liu, California Institute of Technology. Tel Das Einschalten eines Cognitive Assistant für blind mit erweiternde Realität.
* 15:15-16:15: Kaffeepause und Demos.
* 16:15-16:45: **Keynote Talk**: Prof. Kristen Grauman, University of Texas bei Austin/Facebook AI Research. Tel Interaktion zwischen Benutzer und Objekt in Video der ersten Person.
* 16:45-17:15: Mündliche Präsentationen:
    * Die Registrierung machte eine einfache, orthopädische Navigation mit hololens. F. Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. snebeker, M. Farshad, P. furnstahl.
    * Erlernen von Stereo durchlaufen mit einem hololens. MICHA. Zhan, Y. Peer. PEP, O. Ulusoy.
* 17:15-17:30: Abschließende Hinweise.
