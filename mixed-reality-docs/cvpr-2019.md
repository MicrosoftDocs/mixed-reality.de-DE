---
title: Maschinelles Sehen Anwendungen für den Mixed Reality-Headsets-Workshop bei CVPR 2019
description: Übersicht und Zeitplan des Maschinelles Sehen Anwendungen für den Remix-Workshop für gemischte Realität, die auf der CVPR-Konferenz am 2019. Juni geliefert werden.
author: fbogo
ms.author: febogo
ms.date: 1/9/2019
ms.topic: article
keywords: Event, Research Mode, CVPR, Maschinelles sehen, Forschung, hololens
ms.openlocfilehash: 55fbeea1f1293c7df5eae489b6504851bf6bca7f
ms.sourcegitcommit: 6bc6757b9b273a63f260f1716c944603dfa51151
ms.translationtype: MT
ms.contentlocale: de-DE
ms.lasthandoff: 11/01/2019
ms.locfileid: "73435615"
---
# <a name="computer-vision-applications-for-mixed-reality-headsets"></a>Maschinelles Sehen von Anwendungen für Mixed Reality-Headsets

Organisiert in Verbindung mit [CVPR 2019](https://cvpr2019.thecvf.com/)

Long-Strand (ca)

17. Juni 2019 (Mittag)-Hyatt-Regency F


## <a name="organizers"></a>Organis
* Marc Pollefeys
* Federica Bogo
* Johannes Schönberger
* Osman Ulusoy

## <a name="overview"></a>Übersicht

![Teaser-Bild](images/cvpr2019_teaser2.jpg)

Gemischte Reality-Headsets, wie z. b. Microsoft hololens, werden zu leistungsfähigen Plattformen, um maschinelles sehen-Anwendungen Der hololens Research-Modus ermöglicht Maschinelles sehen auf Geräten durch die Bereitstellung von Zugriff auf alle Rohbild-Sensordaten Ströme, einschließlich der Tiefe und der IR. Da der Research-Modus jetzt seit dem 2018 von Mai verfügbar ist, beginnen wir mit der Entwicklung einiger interessanter Demos und Anwendungen, die für hololens entwickelt werden. 

Das Ziel dieses Workshops besteht darin, Studenten und Forscher zusammenzubringen, die an der Maschinelles sehen für gemischte Reality-Anwendungen interessiert sind. Der Workshop stellt einen Veranstaltungsort für die gemeinsame Nutzung von Demos und Anwendungen bereit und erfährt von einander, um Anwendungen zu entwickeln oder zu portieren. 

Wir empfehlen Übermittlungen zu den Themen der (egozentrischen) Objekterkennung, Hand-und Benutzer Nachverfolgung, Aktivitäts Erkennung, Slam, 3D-Rekonstruktion, Szenen Verständnis, sensorbasierte Lokalisierung, Navigation und mehr.

## <a name="paper-submission"></a>Papier Übermittlung
* Stichtag für Papier Übermittlung: 17. Mai
* Benachrichtigung an Autoren: 24. Mai

Für Papier Übermittlungen sollte die CVPR-Vorlage verwendet werden, und Sie sind auf vier Seiten plus Verweise beschränkt. Außerdem empfehlen wir den Autoren, ein Video zu übermitteln, in dem Ihre Anwendung vorgestellt wird.
Beachten Sie, dass Übermittlungen bereits veröffentlichter Arbeit zulässig sind (einschließlich der Arbeit, die an die Hauptkonferenz CVPR 2019 akzeptiert wird). 

Übermittlungen können auf das CMT- https://cmt3.research.microsoft.com/CVFORMR2019 hochgeladen werden.

Im Workshop wird eine Teilmenge der Dokumente zur mündlichen Präsentation ausgewählt. Wir empfehlen jedoch allen Autoren dringend, ihre Arbeit während der Demo Sitzung zu präsentieren.


## <a name="schedule"></a>Zeitplan
* 13:30-13:45: Begrüßungs-und Öffnungs Hinweise.
* 13:45-14:15: **Keynote Talk**: Prof. Marc Pollefeys, ETH Zürich/Microsoft. Title: egozentrische Maschinelles sehen auf hololens.
* 14:15-14:45: **Keynote Talk**: Prof. Kris Kitani, Carnegie Mellon University. Title: die egozentrische Aktivität und das darstellen von Vorhersagen.
* 14:45-15:15: **Keynote Talk**: Dr. Yang Liu, California Institute of Technology. Title: Schalten Sie einen Cognitive Assistant für blind mit erweiternde Realität ein.
* 15:15-16:15: Kaffeepause und Demos.
* 16:15-16:45: **Keynote Talk**: Prof. Kristen Grauman, University of Texas bei Austin/Facebook AI Research. Title: Interaktion von Menschen Objekten im Video der ersten Person.
* 16:45-17:15: mündliche Präsentationen:
    * Die Registrierung machte eine einfache, orthopädische Navigation mit hololens. c. Liebmann, S. Roner, M. von Atzigen, F. Wanivenhaus, C. Neuhaus, J. Spirig, D. Scaramuzza, R. Sutter, J. snebeker, M. Farshad, P. furnstahl.
    * Erlernen von Stereo durchlaufen mit einem hololens. Micha. Zhan, Y. Peer. PEP, O. Ulusoy.
* 17:15-17:30: Abschließende Hinweise.
